%++++++++++++++++++++++++++++++++++++++++
% Don't modify this section unless you know what you're doing!
\documentclass[letterpaper,11pt]{article}
\usepackage{natbib}
\bibliographystyle{unsrtnat}
\usepackage{tabularx} % extra features for tabular environment
\usepackage{amsmath}  % improve math presentation
\usepackage{graphicx} % takes care of graphic including machinery
\usepackage[margin=1in,letterpaper]{geometry} % decreases margins
%\usepackage{cite} % takes care of citations
\usepackage[final]{hyperref} % adds hyper links inside the generated pdf file
\hypersetup{
	colorlinks=true,       % false: boxed links; true: colored links
	linkcolor=blue,        % color of internal links
	citecolor=blue,        % color of links to bibliography
	filecolor=magenta,     % color of file links
	urlcolor=blue         
}
%+++++++++++++++++++++++++++++++++++++++
\begin{document}

\title{Topics in Advanced Multimedia Technologies \\\textbf{On How  Video Compression Quality Affects Face And Pedestrian Detection Performance}}
\author{Ivachev Fedor 2021380027, Lin Yuhong 2021380013, Your name + ID}
\date{December 16, 2021}
\maketitle

\begin{abstract}


Typically, visual intelligent systems are trained and tested on high quality datasets, however, in practical video surveillance applications, video frames cannot be assumed to be of high quality due to video encoding, transmission and decoding associated with the limited bandwidth to which the cameras are connected, and the limitations of the cameras themselves, which usually write video in H.264 (Wiegand et al., 2003). Video streaming is also affected by limited network bandwidth, and usually is performed with a low bitrate. Video compression algorithms are based on video quality indicators, which are designed to take into account the capabilities of the human visual system, that is, their task is to show the best picture for a person, with the minimum allowable video size. At the same time, when the video bitrate is lowered, object recognition works with less accuracy. In this article, we evaluate 3 modern neural network models for pedestrian and face detection at various levels of video compression. We show that existing detectors are susceptible to quality distortions arising from compression artifacts during video capture. We also provide research that avoids this disadvantage.
\end{abstract}

\section{Introduction}

Smart surveillance cameras surround us everywhere. We unlock our phone and laptop using a face scanner built into the device, our car signals about cars that are at a critical distance to it, warning us of danger, we pay for purchases in stores by looking at the sensor. In addition, on the street, in the subway, in various government buildings, there are cameras that continuously transmit the image from them to the server for processing and detecting people in the image, monitor the behavior, activities, or other changing information for the purpose of protecting people and infrastructure. These images have been used for a very long time in algorithms for recognizing people not only by their face, but also by their gait.

Nevertheless, most of these video analysis systems are subject to serious risks: with a sharp decrease in network bandwidth, less information needs to be transmitted, namely, the video bitrate is reduced. In our study, we test the hypothesis of a decrease in the accuracy of object recognition, namely, faces and pedestrians in a video with a decrease in its bitrate, and analyze the results obtained.


\section{Previous work}

In Bagdanov et al. [2] method special frame areas are selected to be maintained at higher visual quality, while the rest was smoothed. Used as pre-processing step to H.264 encoding, it decreased bitrate, while increasing the performance of pedestrian detection compared to videos compressed only with H.264. However, videos are still encoded without using this approach.

Recently,  it was also considered that image degradations affected networks trained for face recognition, lowering their accuracy (Karahan et al., 2016). H.264 compression also degrades image quality.

 for video encoding, which is a lossy compression technique. H.264 exploits spatial redundancy within images and temporal redundancy in videos to achieve appealing compression ratios, making it a widely accepted standard for video transmission for a myriad of ap- plications. A video consists of images; an image is divided into slices and blocks. A block is a square part (16×16, 8×8 and 4×4) of the images. H.264 is a block based coder/decoder, meaning that a se- ries of mathematical functions are applied on indivi- dual blocks to achieve compression and decompres- sion (Juurlink et al., 2012). We study the effect of this degradation in quality on object detection algorithms.








The \textit{Biot-Savart Law} is useful in ... [2].

A \textcolor{blue}{Helmholtz coil} is a device for ... and was named after ... These coils were widely used in ... to produce \underline{uniform magnetic fields} ...



\section{Experiments}

In order to evaluate the performance of object detection algorithms, many valuable benchmarks have been proposed in the literature. Among these are COCO (\cite{lin2015microsoft}), PASCAL VOC 2007 and 2012 (\cite{article2}). Despite the fact that they are mainly used to test object detection on single images, and not on video, nevertheless, we decided to use the second of them to test our hypothesis, since we checked manually by looking at a snippet of video frame by frame and making sure that that this method gives results that correspond to the real picture. 

The false negative rate – also called the miss rate – is the probability that a true positive will be missed by the test. It’s calculated as $\frac{FN}{FN+TP}$, where $FN$ is the number of false negatives and $TP$ is the number of true positives ($FN+TP$ being the total number of positives).

In PASCAL benchmark, a match where intersection divided by the union of two rectangles considering recognized objects is higher than 0.5 (see Figure \ref{fig1}) and if that ground-truth object has not been already used (to avoid multiple detections of the same object) is considered a True Positive (\ref{eq1}). 

\begin{equation} \label{eq1} % the label is used to reference the equation
IoU =\frac{area(B_{gt} \bigcap B_{d})}{area(B_{gt} \bigcup B_{d})}
\end{equation}

\begin{figure}[ht] 
        \centering \includegraphics[width=0.4\columnwidth]{figure.png}
        \caption{\label{fig1} IoU or Intersection over Union for detected objects.
        }
\end{figure}

Nevertheless, if the number of objects (namely faces or pedestrians) on the video is relatively large, and including intersecting bounding boxes for the recognized faces in the ground truth images, PASCAL benchmark may work with errors, counting additional false negatives.

\section{Results and Analysis}



Describe all your results after presenting them. Include tabulated data set, larger tables can also be presented on the Appendix. Here's an example to insert a table -  Table~\ref{table1} is below:

\begin{table}[ht]
\begin{center}
\caption{Every table needs a caption. Note that the table caption is on top of the table! Note the consistency of precision of table values; do not forget the errors, labels, variables, and units.}
\label{table1} 
\begin{tabular}{ccc} %change to cc for 2 columns
\hline
\multicolumn{1}{c}{Distance, $d$ (km) } & \multicolumn{1}{c}{Voltage, $V\ (\pm 0.05$ V)} & \multicolumn{1}{c}{Current, $I$\ (mA $\pm 5$\%)}\\
\hline
1.2 $\pm$ 0.2 &  0.30 & 20 \\
1.6 $\pm$ 0.4 &  0.21 & 30 \\
2.5 $\pm$ 0.1 &  0.18 & 40 \\
5.9 $\pm$ 0.2 &  0.13 & 50 \\
\hline
\end{tabular}
\end{center}
\end{table}



The main difference between detecting objects on videos and photos is that the movements of the objects can be used to gain extra information for their detection and classification. The very first task in video analysis is the estimation of motion or optical flow. Optical flow refers to the vector field of the apparent movement of pixels between frames. At the same time, Motion Estimation does the same task and is the first module of a video encoder. Since consecutive video frames include large amounts of same data, and differ only by moving objects, a next frame can be reconstructed from the previous one. The motion vectors used for the reconstruction can be also used for locating the object in the next frame, if an object was recognized successfully in the original one. 

However, preparing datasets for Optical Flow based object detectors classifiers is a hard task, and not many of such datasets exist. One practical dataset currently in use is the KITTY dataset for self-driving cars. The approach of using Optical Flow based detectors provides better accuracy than our solution, but requires more time for creating dataset.

The other disadvantage of our solution is running time, which can be decreased slightly by using Visual Object Tracking (VOT).For example, every 5 frames a detector is applied, and in the intermediate frames, the face is tracked as a small fragment through a pattern matching method. This approach is used in GOTURN (Generic Object Tracking Using Regression Networks).


\section{Conclusion}
This section should be brief, concise, but complete. Directly answer your objectives, state your findings with errors, and conclude whether or not you were successful. Briefly explain if not successful.
\cite{article1}


\bibliographystyle{plainnat}
\bibliography{references.bib}

\appendix

\section*{Appendix: Velocity measurements}

Below is an example large table; include mathematical derivations here as well.
\begin{table}[ht]
\begin{center}
\caption{Every table needs a caption.}
\label{table2} 
\begin{tabular}{cc} 
\hline
\multicolumn{1}{c}{distance (m)} & \multicolumn{1}{c}{V (km s$^-1$)} \\
\hline
0.0044151 &   0.0030871 \\
0.0021633 &   0.0021343 \\
0.0003600 &   0.0018642 \\
0.0023831 &   0.0013287 \\
0.0044151 &   0.0030871 \\
0.0021633 &   0.0021343 \\
0.0003600 &   0.0018642 \\
0.0023831 &   0.0013287 \\
0.0044151 &   0.0030871 \\
\hline
\end{tabular}
\end{center}
\end{table}

See the inserted full page plot below (Figure \ref{fig2}) for reference (sample data only, past student submission).

\begin{figure}[ht] 
        \centering \includegraphics[width=.9\columnwidth]{Capture}
        \caption{\label{fig2}Every figure MUST have a caption. Note that the figure caption is below the figure! Do not foget to include error bars on your plots.
        }
\end{figure}


\end{document}
